{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"pytorch_pretrained_bert.tokenization\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd().replace('scripts','')\n",
    "reviews = pd.read_csv(path + 'data/Movies_Reviews.csv')\n",
    "features = reviews['Plot']\n",
    "targets = reviews['averageRating']\n",
    "features = features.str.lower()\n",
    "features = features.str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-c6249bf00ac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;34m'bert-base-uncased'\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m   \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#input_ids = torch.tensor( [   tokenizer.tokenize( x ) for x in features.values[:10]  ] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4040\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4042\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-c6249bf00ac9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;34m'bert-base-uncased'\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m   \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#input_ids = torch.tensor( [   tokenizer.tokenize( x ) for x in features.values[:10]  ] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madded_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    648\u001b[0m             return sum((self._tokenize(token, **kwargs) if token not \\\n\u001b[1;32m    649\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                     else [token] for token in tokenized_text), [])\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    648\u001b[0m             return sum((self._tokenize(token, **kwargs) if token not \\\n\u001b[1;32m    649\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                     else [token] for token in tokenized_text), [])\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mstart_new_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_is_punctuation\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    465\u001b[0m             (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "#from pytorch_pretrained_bert import BertTokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained(  'bert-base-uncased'  )\n",
    "model = BertModel.from_pretrained( \"bert-base-uncased\" )\n",
    "input_ids = features.apply( lambda x:  tokenizer.encode(   \" \".join( tokenizer.tokenize( x )[:450]   ) ))     \n",
    "#input_ids = torch.tensor( [   tokenizer.tokenize( x ) for x in features.values[:10]  ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9e6f5539bbac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m)\u001b[0m   \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/transformers/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4040\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4042\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9e6f5539bbac>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m)\u001b[0m   \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "#input_ids = features.apply( lambda x: tokenizer.convert_tokens_to_ids( \" \".join( x )   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             the earliest known adaptation of the classic f...\n",
       "1             alice follows a large white rabbit down a rabb...\n",
       "2             the film opens with two bandits breaking into ...\n",
       "3             the film opens with two bandits breaking into ...\n",
       "4             joe is an impoverished new york newsboy who li...\n",
       "                                    ...                        \n",
       "28916         albus dumbledore minerva mcgonagall and rubeus...\n",
       "28917         the story starts in the 9th century the king r...\n",
       "28918         shiva prithviraj sukumaran is an energetic and...\n",
       "28919         gaja darshan and krishna are friends gaja goes...\n",
       "token_bert    0        [the, earliest, known, adaptation, of...\n",
       "Name: Plot, Length: 28921, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2182,  2003,  2070,  3793,  2000,  4372, 16044,   102]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as', '##das', '##das']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\" asdasdas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0271e-01, -1.6718e-01,  6.3051e-01,  1.6157e-01, -1.7848e-01,\n",
      "         -1.1350e-01,  2.6125e-01,  2.2973e-01,  7.6998e-01, -9.3058e-01,\n",
      "          6.4805e-01, -5.1526e-01,  9.3576e-01, -4.7027e-01,  8.9680e-01,\n",
      "          5.0346e-02,  3.3997e-01, -1.7294e-01,  9.4080e-02,  1.0966e-01,\n",
      "          7.6977e-01,  6.5345e-01,  5.7147e-01,  2.2348e-01,  2.4440e-01,\n",
      "         -5.8442e-01, -2.8376e-01,  9.2562e-01,  8.7001e-01,  6.8228e-01,\n",
      "         -4.0243e-01,  1.7420e-01, -9.7629e-01, -1.2137e-01, -3.7504e-02,\n",
      "         -8.6969e-01,  4.3412e-02, -4.5852e-01, -8.4825e-02,  2.5484e-01,\n",
      "         -8.5882e-01,  8.6413e-03,  7.9209e-01, -6.0185e-01,  1.6298e-01,\n",
      "         -3.6821e-02, -5.8777e-01, -1.6710e-01, -8.0094e-01, -9.0181e-01,\n",
      "         -7.7103e-01, -6.0810e-01, -1.4584e-01,  9.3211e-02, -6.8698e-02,\n",
      "          3.8265e-01, -1.2101e-01,  1.5802e-01,  1.8765e-02, -7.8243e-02,\n",
      "         -2.6540e-01,  2.6653e-02,  4.5865e-02, -6.0141e-01, -8.4735e-01,\n",
      "         -8.5521e-01, -5.9323e-02, -2.2589e-01, -6.7482e-02, -1.0468e-01,\n",
      "          3.7739e-01,  3.0201e-01,  4.0535e-01, -9.0903e-01, -8.5537e-01,\n",
      "          1.3106e-01, -2.3882e-01,  8.5412e-01,  2.1608e-01, -9.5311e-01,\n",
      "          4.0768e-01, -7.5998e-01,  3.0282e-01,  8.0975e-01, -5.3604e-01,\n",
      "         -7.9798e-01, -1.5941e-01, -6.8017e-03, -9.6048e-01, -3.3065e-02,\n",
      "          3.2907e-01,  1.5524e-01,  3.6888e-02,  3.2046e-01,  1.9582e-01,\n",
      "         -4.0410e-01,  9.5052e-02,  4.4619e-01, -2.5500e-01, -2.0205e-01,\n",
      "          7.7694e-02, -3.4845e-03, -1.9998e-02, -3.2212e-03,  1.6807e-01,\n",
      "         -2.0458e-02, -4.1565e-01,  3.7582e-01, -5.3462e-01,  2.8029e-01,\n",
      "          3.0802e-01, -1.4993e-01, -5.1428e-02, -7.8704e-01,  1.3481e-01,\n",
      "         -7.6854e-02, -9.4018e-01, -2.1990e-01, -9.7112e-01,  5.4613e-01,\n",
      "          3.5807e-02, -1.9260e-01,  8.7200e-01,  1.0741e-01,  3.3369e-02,\n",
      "         -5.0903e-02,  9.2530e-01, -9.0130e-01,  4.7973e-01,  5.8605e-02,\n",
      "          1.0208e-01, -1.4764e-01, -9.3080e-01, -9.3144e-01,  3.6282e-01,\n",
      "          8.2983e-01,  1.0516e-01,  8.4729e-01, -6.4488e-02,  8.6826e-01,\n",
      "          7.0744e-01,  2.6989e-02, -7.2995e-01, -8.8238e-02,  3.7603e-01,\n",
      "         -1.0214e-01, -4.2431e-01, -7.3064e-02,  6.5015e-02, -7.1852e-01,\n",
      "         -1.9973e-01,  9.0386e-03,  7.8126e-01, -7.8676e-01, -9.8387e-02,\n",
      "          9.3269e-01,  2.1540e-01,  8.7890e-01,  7.2112e-01,  1.6024e-02,\n",
      "         -1.9743e-01,  5.9901e-01,  1.4468e-01, -1.3070e-01,  1.0341e-01,\n",
      "          3.0931e-02, -6.9372e-01,  1.0595e-01, -4.4355e-01,  4.7848e-01,\n",
      "         -4.3810e-02,  8.9688e-02,  3.3766e-01, -9.5193e-01,  1.1271e-01,\n",
      "          1.2022e-01,  9.3736e-01,  4.9531e-01, -5.9047e-02, -8.6232e-01,\n",
      "          5.2263e-02, -5.2682e-01, -9.3209e-01,  9.4947e-01,  6.3127e-04,\n",
      "          7.1957e-02, -4.1961e-02, -1.7979e-02, -5.1027e-01, -1.9933e-01,\n",
      "          1.1893e-01,  4.0906e-01, -6.3899e-01,  1.8312e-01, -2.6478e-01,\n",
      "         -2.2164e-01, -2.4560e-01, -3.0197e-02, -2.0034e-01, -1.9282e-01,\n",
      "         -7.7272e-02,  9.2584e-01,  2.2921e-01,  2.8089e-01, -5.7118e-01,\n",
      "          1.3469e-01, -7.7172e-01, -4.5972e-01, -8.8486e-02, -1.3273e-01,\n",
      "          1.3529e-01,  9.5601e-01, -5.6690e-01,  1.2028e-01, -5.9601e-01,\n",
      "         -9.5186e-01, -7.9706e-02, -5.5038e-01,  1.3181e-01, -3.2609e-01,\n",
      "          9.6105e-02, -2.7753e-01, -8.4907e-01,  8.4747e-02, -7.0146e-01,\n",
      "         -7.5890e-01,  1.4907e-01, -5.5976e-02,  1.1197e-01,  9.3860e-02,\n",
      "          8.9059e-01, -2.0618e-01, -4.1601e-01,  3.9020e-02,  9.4750e-01,\n",
      "          4.2170e-02, -7.4431e-01,  5.6477e-01, -1.5020e-01,  4.8537e-01,\n",
      "         -1.4112e-01,  8.2400e-01, -4.0426e-01,  1.1730e-01, -7.9464e-01,\n",
      "          2.1469e-01,  5.9286e-02,  8.2941e-01, -1.3856e-01, -7.3593e-01,\n",
      "         -8.3539e-01,  2.2203e-01,  1.3673e-01,  6.6572e-01, -1.6313e-01,\n",
      "          7.3602e-02, -9.2578e-01, -9.1211e-01, -5.2249e-01,  2.7566e-01,\n",
      "         -9.5703e-01, -4.0620e-01,  2.2875e-01,  4.9156e-01, -2.1349e-01,\n",
      "         -4.3325e-02, -9.5261e-01,  4.0003e-01, -2.4819e-02,  6.4750e-01,\n",
      "          5.9350e-02, -3.0297e-01,  1.8130e-01, -9.5093e-01, -1.4200e-01,\n",
      "          1.3489e-01,  7.6780e-01, -1.2549e-01, -8.1429e-01,  3.3207e-01,\n",
      "          2.4857e-01,  8.8535e-02,  7.9626e-01,  5.1777e-01,  8.9192e-01,\n",
      "          9.1869e-01,  7.5330e-01,  2.6724e-02, -7.1584e-01, -4.3121e-01,\n",
      "          9.3715e-01,  2.7603e-01, -8.5739e-01, -7.3468e-01,  3.9758e-02,\n",
      "          3.7329e-01, -9.0645e-01, -7.6766e-02,  1.8622e-01, -7.0018e-01,\n",
      "         -7.9694e-01,  9.3166e-01,  4.8169e-01, -9.3603e-01,  1.3667e-01,\n",
      "          7.4602e-01, -2.5916e-01, -5.9042e-01, -4.3435e-02,  8.8816e-01,\n",
      "          1.9841e-01,  4.4407e-01, -1.6750e-01,  4.4546e-01,  2.2701e-01,\n",
      "         -5.8627e-01,  6.2792e-01,  8.5920e-02,  3.7050e-01,  7.3316e-02,\n",
      "         -2.4716e-01, -8.7060e-01, -7.1861e-02, -6.8227e-02, -3.1635e-01,\n",
      "         -9.3351e-01, -1.3975e-01, -7.4538e-01,  1.1686e-01,  1.3549e-01,\n",
      "         -6.2938e-02, -3.5250e-01, -2.8902e-02, -2.4806e-01,  1.0506e-01,\n",
      "          3.6035e-01, -7.4034e-01, -1.3732e-01,  3.2794e-01, -5.1623e-01,\n",
      "          4.8271e-01, -9.4616e-01,  8.9845e-01, -6.8118e-02, -9.1268e-01,\n",
      "          9.2279e-01,  2.8692e-02, -7.7071e-01,  3.7324e-01, -5.3407e-02,\n",
      "         -2.0046e-02,  8.0924e-01,  1.4121e-01, -9.6324e-01, -2.7805e-01,\n",
      "          1.0258e-01, -1.9603e-01, -2.2223e-01,  9.0521e-01,  6.8197e-02,\n",
      "          7.3595e-01,  4.6804e-01,  9.6863e-01, -9.7326e-01,  9.5329e-03,\n",
      "         -4.6417e-01, -9.1432e-01,  8.9434e-01,  9.2976e-01,  1.7883e-01,\n",
      "         -1.9455e-01, -1.8048e-01,  6.1985e-01,  6.6284e-02, -3.6731e-01,\n",
      "         -7.3925e-03,  2.2394e-01,  5.3264e-02,  7.7553e-01,  2.0498e-01,\n",
      "         -1.5643e-01, -5.2968e-03,  4.8509e-01,  6.2768e-01, -1.5332e-01,\n",
      "          8.3349e-02, -1.0648e-01, -8.2753e-02, -1.0488e-01, -3.8858e-01,\n",
      "         -8.5613e-01,  1.1448e-01,  9.1680e-01,  9.2675e-02, -1.7634e-01,\n",
      "          6.8851e-01, -1.7988e-01,  4.9655e-02,  7.8130e-02,  7.2594e-02,\n",
      "         -4.8328e-03, -4.2768e-01, -6.3552e-01, -4.7450e-01, -9.6533e-01,\n",
      "          5.7078e-01,  1.1578e-01, -1.4643e-01,  5.3586e-01, -1.9947e-01,\n",
      "          1.3209e-01, -2.6827e-01, -7.1710e-01, -3.8002e-03,  1.7485e-01,\n",
      "         -8.7972e-01,  9.2255e-01,  6.6151e-02,  3.2891e-01,  2.1490e-01,\n",
      "          9.0907e-01, -2.6783e-01, -9.1823e-02,  1.3012e-01, -8.7406e-01,\n",
      "         -4.7290e-02, -8.9770e-01,  9.3966e-01, -7.3047e-01,  6.9741e-02,\n",
      "          1.1907e-01, -5.9420e-02,  8.6962e-01, -6.4876e-01,  2.6459e-01,\n",
      "          4.7904e-01,  4.8370e-01, -3.0252e-01, -4.2703e-01, -2.4369e-01,\n",
      "          2.2589e-01,  8.0414e-01, -6.3115e-02,  1.2429e-03, -8.6188e-01,\n",
      "         -8.5448e-01, -2.2668e-01, -4.8428e-01, -9.3839e-01,  5.0995e-01,\n",
      "         -1.5051e-02, -5.5259e-02, -4.8287e-01, -1.6524e-01, -3.4930e-01,\n",
      "         -5.8452e-01,  3.9750e-02, -9.1255e-01,  8.1589e-01, -5.6304e-03,\n",
      "          1.9694e-01, -8.0649e-02,  3.5232e-01, -7.6249e-01,  8.2949e-01,\n",
      "          9.4720e-02, -3.1765e-01, -1.3342e-01, -4.9557e-01,  1.4085e-01,\n",
      "         -2.5461e-01,  5.4800e-01,  1.1573e-02,  8.4109e-01,  4.1896e-02,\n",
      "         -5.7370e-01,  5.8284e-01,  3.1921e-01, -1.0956e-01,  1.0900e-01,\n",
      "          4.4681e-01,  1.8555e-01,  7.0125e-01,  9.0448e-01,  5.3275e-01,\n",
      "         -1.9964e-01,  2.0485e-01, -2.8137e-01, -4.6773e-01,  6.2481e-01,\n",
      "          7.3014e-03,  2.5363e-01, -6.9119e-02,  1.1768e-01, -4.7211e-03,\n",
      "          1.0592e-01, -1.6468e-02, -2.3943e-01, -7.0667e-02, -2.1603e-01,\n",
      "         -2.9434e-01,  8.9898e-01,  1.3423e-02,  2.1010e-01, -9.6070e-01,\n",
      "          5.7209e-01, -1.7516e-01,  8.3120e-01,  8.4388e-01, -2.8715e-02,\n",
      "          3.7241e-02, -1.5788e-01,  7.8626e-02, -6.4899e-02, -2.4176e-02,\n",
      "         -4.7023e-02, -5.7669e-02,  1.0054e-01,  9.1179e-01, -2.9212e-01,\n",
      "         -9.6042e-01, -5.4502e-01, -6.1695e-02, -8.2944e-01,  6.2944e-01,\n",
      "          9.6880e-02, -6.5761e-02, -1.5025e-02, -1.8952e-02,  1.0922e-01,\n",
      "         -3.1936e-02, -9.4510e-01, -3.1126e-03,  1.2509e-01,  9.4318e-01,\n",
      "         -1.2520e-01, -3.4433e-01, -8.8490e-01, -6.2488e-01, -5.3221e-01,\n",
      "          7.4127e-01, -8.8763e-01,  9.3716e-01, -8.9976e-01, -9.7430e-02,\n",
      "          6.2758e-01, -8.1562e-02, -7.9281e-01, -1.4699e-02, -8.5961e-02,\n",
      "         -7.6884e-02, -6.2266e-02,  4.0300e-02, -9.0335e-01, -2.1351e-01,\n",
      "         -2.4730e-01,  5.5556e-03,  3.5043e-03, -4.3960e-01,  3.7947e-01,\n",
      "          2.5983e-01, -3.1655e-01, -1.2922e-01,  2.0246e-01,  8.2062e-02,\n",
      "          3.0207e-01, -1.3831e-01,  9.4393e-02, -8.5613e-02, -1.2806e-01,\n",
      "         -9.0850e-01,  4.5990e-02, -2.3085e-02, -6.8844e-01,  5.6395e-01,\n",
      "         -8.2631e-01,  6.9702e-02, -8.2251e-01, -5.2416e-02,  7.4384e-01,\n",
      "          4.7986e-01, -5.9044e-01, -5.0013e-01,  8.4952e-01,  8.9799e-01,\n",
      "          7.4931e-01, -4.5829e-02,  7.2451e-01, -6.7122e-01,  9.1733e-02,\n",
      "         -5.3090e-02,  2.9861e-01,  3.7573e-01,  4.4166e-01, -1.5920e-01,\n",
      "          8.6407e-01, -7.9588e-04,  1.3937e-01, -5.5934e-01,  3.4094e-02,\n",
      "         -8.9148e-02,  8.9178e-01,  6.2056e-03, -9.1102e-01,  1.6601e-01,\n",
      "         -3.4261e-01, -7.6399e-01,  1.2992e-01,  6.3199e-02, -3.8903e-01,\n",
      "          2.5319e-01,  8.7691e-01,  9.6046e-02, -4.2067e-01,  3.5742e-01,\n",
      "         -1.8909e-01, -1.5334e-01, -5.9422e-03, -6.1125e-01,  9.6556e-01,\n",
      "          1.8889e-01,  4.0554e-01, -4.9680e-01,  5.0348e-02,  9.1297e-01,\n",
      "          1.9444e-01, -1.4126e-01, -4.6851e-02,  8.0732e-01,  3.8737e-03,\n",
      "         -8.7359e-01,  3.2237e-01, -7.8888e-01, -1.8644e-01, -7.6769e-01,\n",
      "         -4.8312e-02,  2.1955e-01,  8.3122e-01,  2.5156e-02,  8.7902e-01,\n",
      "          7.5708e-01, -1.3472e-01,  4.7971e-01,  8.7966e-01,  7.3977e-02,\n",
      "         -9.1518e-01, -9.4111e-01, -9.5749e-01,  1.5904e-02,  1.3634e-01,\n",
      "         -5.4207e-02,  1.1528e-01,  9.7939e-02, -8.9790e-03,  5.9168e-02,\n",
      "         -9.0383e-01,  7.9649e-01,  1.4141e-01, -6.5500e-01,  9.2177e-01,\n",
      "          3.6786e-02,  4.1730e-02,  1.6369e-01, -9.4848e-01, -5.1018e-01,\n",
      "         -8.4571e-02,  1.3479e-01,  4.6206e-01,  2.5843e-01,  7.1082e-01,\n",
      "         -1.4666e-01, -2.0119e-01, -4.3415e-01,  3.1659e-01, -8.4198e-01,\n",
      "         -9.7406e-01,  3.5968e-01,  8.9012e-01, -1.8150e-01,  9.3461e-01,\n",
      "         -6.1665e-01,  1.0523e-02,  6.1604e-01,  4.9350e-02,  1.9871e-02,\n",
      "          3.4764e-01,  1.2296e-01, -5.1793e-02,  4.9904e-01,  8.1471e-01,\n",
      "          5.0703e-01,  9.2683e-01,  6.6793e-01,  4.8418e-02,  5.7297e-01,\n",
      "          1.8958e-01,  5.6660e-01, -8.8540e-01, -3.7817e-03,  3.1336e-01,\n",
      "          4.8829e-02, -3.0818e-02, -1.7234e-01, -5.8439e-01, -2.8053e-01,\n",
      "         -9.6902e-02,  3.5087e-01, -2.0172e-01,  1.1409e-01, -1.5184e-01,\n",
      "          2.0658e-03,  1.0311e-02, -4.7370e-02,  3.5689e-01, -1.0592e-01,\n",
      "          8.2880e-01,  4.8715e-01, -1.1548e-01,  3.6989e-01, -1.7309e-02,\n",
      "          8.0662e-01, -7.7769e-01, -1.6142e-01,  3.3209e-02,  7.0618e-01,\n",
      "          1.8779e-01, -2.6453e-02,  4.3485e-01, -3.4731e-01,  9.5214e-02,\n",
      "         -1.5946e-02, -3.2633e-01,  2.1655e-01,  1.5726e-01, -3.6235e-01,\n",
      "         -2.9340e-01, -1.0797e-01,  1.3230e-01,  7.1172e-01,  6.3059e-01,\n",
      "          8.8476e-01, -3.2148e-02, -1.0782e-01, -3.9635e-02, -2.1888e-01,\n",
      "         -8.5495e-01, -1.0217e-01,  2.7221e-01, -3.1589e-01,  5.8180e-01,\n",
      "          7.7384e-03,  6.2628e-01, -7.4715e-01, -1.5311e-01,  3.6713e-01,\n",
      "         -3.9712e-01, -6.6803e-02,  3.5916e-01,  2.7997e-01,  8.5922e-01,\n",
      "         -4.6740e-01,  6.1786e-01,  5.7559e-01,  7.0447e-01,  2.3274e-01,\n",
      "         -5.0302e-02, -3.2575e-01,  7.1489e-01]], grad_fn=<TanhBackward>)\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "# Transformers has a unified API\n",
    "# for 8 transformer architectures and 30 pretrained weights.\n",
    "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
    "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased')]\n",
    "\n",
    "# To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`\n",
    "\n",
    "# Let's encode some text in a sequence of hidden-states using each model:\n",
    "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
    "    # Load pretrained model/tokenizer\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    # Encode text\n",
    "    input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n",
    "\n",
    "    # Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n",
    "    BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
    "                          BertForSequenceClassification, BertForMultipleChoice, BertForTokenClassification,\n",
    "                          BertForQuestionAnswering]\n",
    "\n",
    "    BERT_MODEL_CLASSES = [ BertModel ]\n",
    "    # All the classes for an architecture can be initiated from pretrained weights for this architecture\n",
    "    # Note that additional weights added for fine-tuning are only initialized\n",
    "    # and need to be trained on the down-stream task\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    for model_class in BERT_MODEL_CLASSES:\n",
    "        # Load pretrained model/tokenizer\n",
    "        #model = model_class.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Models can return full list of hidden-states & attentions weights at each layer\n",
    "        model = model_class.from_pretrained(pretrained_weights,\n",
    "                                            output_hidden_states=True,\n",
    "                                            output_attentions=False)\n",
    "        input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\n",
    "        all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "\n",
    "        print( all_hidden_states   )\n",
    "        print( all_hidden_states.shape )\n",
    "        # Models are compatible with Torchscript\n",
    "        #model = model_class.from_pretrained(pretrained_weights, torchscript=True)\n",
    "        #traced_model = torch.jit.trace(model, (input_ids,))\n",
    "\n",
    "        # Simple serialization for models and tokenizers\n",
    "        #model.save_pretrained('./directory/to/save/')  # save\n",
    "        #model = model_class.from_pretrained('./directory/to/save/')  # re-load\n",
    "        #tokenizer.save_pretrained('./directory/to/save/')  # save\n",
    "        #tokenizer = BertTokenizer.from_pretrained('./directory/to/save/')  # re-load\n",
    "\n",
    "        # SOTA examples for GLUE, SQUAD, text generation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
